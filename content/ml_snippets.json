{
  "ml_snippets": [
    {
      "title": "Image Classification with CNN",
      "content": "\ud83d\udcf8 Build a Convolutional Neural Network in 5 steps:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# 1. Create model\nmodel = tf.keras.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\n# 2. Compile\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n```\n\n#MachineLearning #DeepLearning #Python",
      "hashtags": [
        "#MachineLearning",
        "#DeepLearning",
        "#Python",
        "#ComputerVision"
      ]
    },
    {
      "title": "NLP Sentiment Analysis",
      "content": "\ud83d\udcac Sentiment Analysis in 5 lines of code:\n\n```python\nfrom transformers import pipeline\n\n# Load sentiment analysis pipeline\nsentiment_analyzer = pipeline('sentiment-analysis')\n\n# Analyze a sample text\nresult = sentiment_analyzer('I love machine learning!')\nprint(f\"Label: {result[0]['label']}, Score: {result[0]['score']:.4f}\")\n# Output: Label: POSITIVE, Score: 0.9998\n```\n\n#NLP #MachineLearning #Python #HuggingFace",
      "hashtags": [
        "#NLP",
        "#MachineLearning",
        "#Python",
        "#HuggingFace"
      ]
    },
    {
      "title": "Time Series Forecasting",
      "content": "\ud83d\udcc8 Time Series Forecasting with LSTM:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\n# Create LSTM model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(time_steps, features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train model\nmodel.fit(X_train, y_train, epochs=100, validation_split=0.2)\n\n# Make predictions\npredictions = model.predict(X_test)\n```\n\n#TimeSeries #LSTM #MachineLearning #DataScience",
      "hashtags": [
        "#TimeSeries",
        "#LSTM",
        "#MachineLearning",
        "#DataScience"
      ]
    },
    {
      "title": "K-Means Clustering",
      "content": "\ud83d\udd0d Customer Segmentation with K-Means Clustering:\n\n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n# Generate sample data\nX = customer_data[['annual_income', 'spending_score']]\n\n# Find optimal number of clusters (Elbow method)\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, random_state=42)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\n# Apply K-Means with optimal cluster number\nkmeans = KMeans(n_clusters=5, random_state=42)\nclusters = kmeans.fit_predict(X)\n```\n\n#Clustering #DataScience #MachineLearning",
      "hashtags": [
        "#Clustering",
        "#DataScience",
        "#MachineLearning",
        "#Python"
      ]
    },
    {
      "title": "Random Forest for Classification",
      "content": "\ud83c\udf32 Random Forest Classifier in action:\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    features, target, test_size=0.25, random_state=42)\n\n# Create and train model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Evaluate\npredictions = rf_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions):.4f}\")\nprint(classification_report(y_test, predictions))\n```\n\n#RandomForest #MachineLearning #Supervised #DataScience",
      "hashtags": [
        "#RandomForest",
        "#MachineLearning",
        "#Supervised",
        "#DataScience"
      ]
    }
  ],
  "code_tips": [
    {
      "title": "Python List Comprehension",
      "content": "\u2728 Python Pro Tip: Use list comprehensions for cleaner code\n\n```python\n# Instead of this:\nsquares = []\nfor i in range(10):\n    squares.append(i**2)\n\n# Do this:\nsquares = [i**2 for i in range(10)]\n\n# With conditions:\neven_squares = [i**2 for i in range(10) if i % 2 == 0]\n```\n\n#Python #Coding #ProgrammingTips",
      "hashtags": [
        "#Python",
        "#Coding",
        "#ProgrammingTips",
        "#CleanCode"
      ]
    },
    {
      "title": "Git Workflow",
      "content": "\ud83d\udd04 Git Workflow Essentials:\n\n```bash\n# Create a feature branch\ngit checkout -b feature/new-feature\n\n# Make your changes\ngit add .\ngit commit -m \"Add new feature\"\n\n# Stay updated with main branch\ngit checkout main\ngit pull\ngit checkout feature/new-feature\ngit rebase main\n\n# Push your changes\ngit push origin feature/new-feature\n```\n\n#Git #DevOps #Programming #VersionControl",
      "hashtags": [
        "#Git",
        "#DevOps",
        "#Programming",
        "#VersionControl"
      ]
    },
    {
      "title": "JavaScript Promises",
      "content": "\u26d3\ufe0f Modern JavaScript: Promises and Async/Await\n\n```javascript\n// Using promises\nfetch('https://api.example.com/data')\n  .then(response => response.json())\n  .then(data => console.log(data))\n  .catch(error => console.error(error));\n\n// Using async/await (cleaner!)\nasync function fetchData() {\n  try {\n    const response = await fetch('https://api.example.com/data');\n    const data = await response.json();\n    console.log(data);\n  } catch (error) {\n    console.error(error);\n  }\n}\n```\n\n#JavaScript #WebDev #AsyncAwait #Promises",
      "hashtags": [
        "#JavaScript",
        "#WebDev",
        "#AsyncAwait",
        "#Programming"
      ]
    }
  ],
  "interview_questions": [
    {
      "title": "ML Interview Question",
      "content": "\ud83d\udcdd Machine Learning Interview Question:\n\nQ: What's the difference between bagging and boosting in ensemble learning?\n\nA: \ud83d\udd39 Bagging (Bootstrap Aggregating):\n- Trains models in parallel on random subsets of data\n- Reduces variance (prevents overfitting)\n- Example: Random Forest\n\n\ud83d\udd39 Boosting:\n- Trains models sequentially, each fixing errors of previous models\n- Reduces bias (improves prediction accuracy)\n- Examples: AdaBoost, Gradient Boosting, XGBoost\n\n#MachineLearning #DataScience #InterviewTips",
      "hashtags": [
        "#MachineLearning",
        "#DataScience",
        "#InterviewTips",
        "#CareerAdvice"
      ]
    },
    {
      "title": "Coding Interview",
      "content": "\ud83e\udde9 Coding Interview: Finding pairs that sum to target\n\n```python\ndef find_pairs(nums, target):\n    # Time: O(n), Space: O(n)\n    seen = set()\n    pairs = []\n    \n    for num in nums:\n        complement = target - num\n        if complement in seen:\n            pairs.append((complement, num))\n        seen.add(num)\n        \n    return pairs\n\n# Example\nnums = [2, 7, 11, 15, 3, 6]\ntarget = 9\nprint(find_pairs(nums, target))  # [(2, 7), (3, 6)]\n```\n\n#CodingInterview #Programming #Algorithms #DataStructures",
      "hashtags": [
        "#CodingInterview",
        "#Programming",
        "#Algorithms",
        "#DataStructures"
      ]
    }
  ]
}